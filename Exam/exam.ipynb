{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4fc29c0d30afb83",
   "metadata": {},
   "source": [
    "# ADA final exam (Fall 2023)\n",
    "\n",
    "This exam consists of 2 parts. Parts are independent from each other.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "\n",
    "\"Friends\" is an American television sitcom that originally aired on NBC from September 22, 1994, to May 6, 2004. Created by David Crane and Marta Kauffman, the show gained immense popularity and has since become a classic in the world of television. The series is set in New York City and revolves around a group of six friends: Ross Geller (David Schwimmer), Rachel Green (Jennifer Aniston), Monica Geller (Courteney Cox), Chandler Bing (Matthew Perry), Joey Tribbiani (Matt LeBlanc), and Phoebe Buffay (Lisa Kudrow). The show explores their personal and professional lives as they navigate the ups and downs of relationships, careers, and the challenges of adulthood.\n",
    "\n",
    "In this exam, we will use a dataset containing all the conversations that occurred over 10 seasons of Friends. We refer to each row in the dataset as an 'utterance.\" The data format of the dataset is as follows\n",
    "\n",
    "- id: `<str>`, the index of the utterance in the format sAA_eBB_cCC_uDDD, where AA is the season number, BB is the episode number, CC is the scene/conversation number, and DDD is the number of the utterance in the scene (e.g. s01_e18_c05_u021).\n",
    "- speaker: `<str>`, the speaker who made the utterance, e.g. Monica Geller\n",
    "- conversation_id: `<str>`, the id of the first utterance in the conversation this utterance belongs. We assume conversations begin at the start of a new scene.\n",
    "- reply_to: `<str>`, the id of the utterance to which this utterance replies. None if the utterance is the first in a conversation.\n",
    "- text: `<str>`, the textual content of the utterance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b039cc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's import some required libraries!\n",
    "import statsmodels.formula.api as smf\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be188a",
   "metadata": {},
   "source": [
    "\n",
    "## Part 1: The one where you find the protagonist (60 pts)\n",
    "\n",
    "A big debate among Friends fans is: who is the show's main character? In this task, your goal is to provide a data-driven answer to this question.\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "**1.1 —** Load the data from the jsonl file `exam1.jsonl` into a pandas dataframe. Then\n",
    " \n",
    " A. Calculate and display the number of distinct speakers in the dataframe.\n",
    " \n",
    " B. Calculate and display the number of conversations (see `conversation_id`).\n",
    " \n",
    " C. Remove all utterances from the dataframe where the `speaker` is \"TRANSCRIPT_NOTE\" or \"#ALL#\". Print the number of rows in the dataframe.\n",
    " \n",
    " D. Create additional columns corresponding to the season (`season`, e.g., season 1 should contain `s01`) and the episode (`episode`, e.g., episode 5 of season 4 should contain `s04_e05`) of each utterance. Print the season and the episode associated with utterance `s10_e18_c11_u019`.\n",
    " \n",
    " E. Create an additional column corresponding to the length of each utterance in terms of the number of characters (`length`). Print the length associated with utterance `s10_e18_c11_u019`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4daa420",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d3bdc7",
   "metadata": {},
   "source": [
    "**1.2** Next, you conduct some basic analyses:\n",
    "\n",
    " A. With `statsmodels`, fit a linear regression `length ~ C(season, Treatment(reference=\"s01\"))`, where length is an integer and season is a categorical variable. Print the regression summary.\n",
    " \n",
    " B. /**Discuss:/** Considering the regression summary:\n",
    "   - What does the intercept in this regression represent? \n",
    "   - What does the coefficient `C(season, Treatment(reference=\"s01\"))[T.s09]` represent? \n",
    "   - Does the average utterance in season 9 contain significantly more characters than in season 1 at the 0.05 significance level? Justify with the regression summary **only**. \n",
    "   - Does the average utterance in season 10 contain significantly more characters than in season 1 at the 0.05 significance level? Justify with the regression summary  **only** .\n",
    "\n",
    " C. Argue visually (i.e., with a plot) that there are 6 main characters in the show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b3c34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eeb47cab",
   "metadata": {},
   "source": [
    "--- \n",
    "**1.3 —** Using `networkx` \n",
    "\n",
    "A. Create a `MultiDiGraph` (directed graph with self loops and parallel edges) where:\n",
    "- Each node $u$ is a character uniquely identified by the `speaker` field.\n",
    "- There is an edge between nodes $u$ and $v$ if $u$ replied to $v$. If an utterance (a row in the dataframe) is said in reply to nobody, then it will not correspond to an edge. Each edge should contain two attributes. Each edge should have two attributes: `season` and `episode`.\n",
    "\n",
    "B. Print the number of nodes and edges in your graph.\n",
    "\n",
    "C. **/Discuss:/** Instead of using multi-edges, what would be another way in which you could capture the number of replies associated with each node pair?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f362612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "130a16f1",
   "metadata": {},
   "source": [
    "---\n",
    "With the graph ready, you set out to investigate who is the true protagonist of Friends.\n",
    "\n",
    "Ignore the graph you generated previously and instead use the graph provided in `exam2.graphml`. Note that this graph may be slightly different from what you generated, but treat it as the ground truth. We provide you with code to load the graph below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a917d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.read_graphml(\"./data/exam2.graphml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad04b50",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ee10e7",
   "metadata": {},
   "source": [
    "**1.4 —** Using the provided MultiDiGraph $G$:\n",
    "\n",
    "A. Calculate the out-degree of each node (also known as out-degree centrality). Please do not use the `nx.out_degree_centrality` function here, as it normalizes the degree. (E.g., if a node has 5 outgoing edges, it should have out-degree 5 according to your code.)\n",
    "\n",
    "B. Calculate the PageRank centrality of each node in $G$. Use the default parameters.\n",
    "\n",
    "C. Print both centrality metrics calculated above for the six main characters of Friends.\n",
    "\n",
    "D. **/Discuss:/** According to the metrics, who is the most important character in Friends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8407dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe9f1601",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1.5 —** **/True or false:/** Considering your answer in **1.4**, are the following statements true or false? Justify your answers. \n",
    "\n",
    "A. \"If we inverted all  edges in the graph such that an edge $(u,v)$ becomes an edge $(v,u)$, the PageRank centrality would remain unchanged.\"\n",
    "\n",
    "B. \"If we removed all outgoing edges from Rachel Green, her PageRank centrality would remain unchanged.\"\n",
    "\n",
    "C. \"If a new node was introduced in the graph, with 1,000 outgoing edges towards each other node, but no incoming edge, it would have the highest PageRank centrality.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf413c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e47355c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1.6 —** Next, you consider how these centrality metrics vary over the course of the seasons.\n",
    "\n",
    "A. Calculate the PageRank and out-degree centrality of the 6 main characters per episode, i.e., for each episode, create a graph containing only the utterances of that episode and calculate the PageRank centrality for this new graph. Print the PageRank and the out-degree of Rachel Green for the first episode of the first season.\n",
    "\n",
    "B. Considering the episode-level out-degree centrality of Phoebe Buffay in season 1 and in season 10, print the mean and the standard error of the mean.\n",
    "\n",
    "C. Create a single plot with 10 inches of width and 4 inches of height. The plot should contain two panels, containing the average PageRank centrality per season of Rachel Green and Ross Geller (Panel A), and the average out-degree per season of Rachel Green and Ross Geller (Panel B). Show 95% confidence intervals in your plot (calculated over the episodes in each season).\n",
    "\n",
    "D. **/Discuss:/** Does the plot support the hypothesis that Rachel was the show's protagonist in all 10 seasons? Explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e09a998",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94b0e5b3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**1.7 —** **/True or false:/** Considering your answer in **1.6** are the following statements true or false? Justify your answer. \n",
    "\n",
    "\n",
    "A. \"In season 7, Rachel Green's episode-level PageRank and out-degree centrality are higher than Ross Geller's. This difference is statistically significant at the 0.05 significance level.\"\n",
    "\n",
    "B. \"Phoebe Buffay's out-degree grew between season 1 and season 10; this implies that other characters spoke less than her in season 10.\"\n",
    "\n",
    "C. \"Phoebe Buffay's PageRank was higher in season 10 than in season 1. This difference is statistically significant at the 0.05 significance level and suggests that the character gained importance over the course of the show.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e48217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "84866967",
   "metadata": {},
   "source": [
    "## Part 2: The one about text similarity (40 pts)\n",
    "\n",
    "Next, you investigate how unique characters are by analyzing what they said throughout the 10 seasons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176a69b8",
   "metadata": {},
   "source": [
    "**2.1 —** Load the dataframe `exam3.jsonl`. This dataframe is similar to `exam1.jsonl`, except:\n",
    "\n",
    "- It has an additional column called `tokens`, containing a sentence list. Each sentence is another list composed of tokens, e.g.,\n",
    "`[['There', \"'s\", 'nothing', 'to', 'tell', '!'], ['He', \"'s\", 'just', 'some', 'guy', 'I', 'work', 'with', '!']]`.\n",
    "- It has an additional column called `episode` containing a unique episode identifier.\n",
    "- It only contains utterances by Phoebe, Rachel, Ross, Joey, Monica, or Chandler (the main characters).\n",
    " \n",
    "Given this dataframe, you will create an episode-level word-frequency matrix for Chandler Bing, one of the main characters.\n",
    "\n",
    "A. Create a list $L$ containing all distinct tokens uttered by Chandler Bing throughout the 10 seasons, sorted in ascending order. Print the 10 first and last elements of the list. \n",
    "\n",
    "B. Create a matrix $X$ with $m$ rows and $n$ columns, where: $n$ is the number of tokens in the list $L$ that you just created, and $m$ is the number of episodes (236). Each position $X_{i,j}$ in this matrix should contain the number of times the character uttered the word $j$ in episode $i$. Print how many times Chandler uttered the token `joey` in the first episode of the first season, as well as the shape of the matrix $X$.\n",
    "\n",
    "C. Transform the matrix $X$ into a TF-IDF matrix $T$, combining the following formula (as seen in class):\n",
    "\n",
    "$$\\text{TF}(i,j) = \\text{number of times the $j$-th word occurs in the $i$-th episode}$$\n",
    "\n",
    "$$\\text{IDF}(j) =  \\log \\frac{\\text{number of episodes}}{\\text{number of episodes in which the $j$-th word occurs}}$$\n",
    "\n",
    "Print the value in the TF-IDF matrix corresponding to Chandler's utterance of the token `joey` in the first episode of the first season.\n",
    "\n",
    "D. **/Discuss:/** Some of the tokens (e.g., `joey`) reference other characters. How may these tokens help a classifier predict which character uttered a sentence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e951338",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b691ae5",
   "metadata": {},
   "source": [
    "---\n",
    "For the remainder of the task, you will use the TF-IDF matrix that we compute below. Note:\n",
    "- This matrix was calculated in a slightly different way: it considers only the 1000 tokens with the highest term frequency.\n",
    "- We provide three useful variables below (`X`, `y`, and `df_tfidf`). \n",
    "    - `X` is a matrix containing the TF-IDF values for the top 1000 tokens, where each row corresponds to a character in an episode. \n",
    "    - `y` indicates which character is responsible for the utterance. Each character has a corresponding number, e.g., 2 for Monica Geller; see dictionary below. \n",
    "    - `df_tfidf` is a dataframe combining `X` with other episode and utterance-level metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e0aebac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape (51312, 1000)\n",
      "y shape (51312,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>speaker</th>\n",
       "      <th>episode</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>990</th>\n",
       "      <th>991</th>\n",
       "      <th>992</th>\n",
       "      <th>993</th>\n",
       "      <th>994</th>\n",
       "      <th>995</th>\n",
       "      <th>996</th>\n",
       "      <th>997</th>\n",
       "      <th>998</th>\n",
       "      <th>999</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Monica Geller</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Joey Tribbiani</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chandler Bing</td>\n",
       "      <td>s01_e01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 1002 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          speaker  episode    0    1    2    3    4    5    6    7  ...  990  \\\n",
       "0   Monica Geller  s01_e01  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "1  Joey Tribbiani  s01_e01  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "2   Chandler Bing  s01_e01  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0   \n",
       "\n",
       "   991  992  993  994  995  996  997  998  999  \n",
       "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[3 rows x 1002 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "df_tfidf = pd.read_json(\"./data/exam3.jsonl\", lines=True)[[\"speaker\", \"episode\", \"text\"]]\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\")\n",
    "X = vectorizer.fit_transform(df_tfidf.text).toarray()\n",
    "print(\"X shape\", X.shape)\n",
    "\n",
    "\n",
    "map_char_to_int = {\n",
    "'Chandler Bing': 0,\n",
    "'Joey Tribbiani': 1,\n",
    "'Monica Geller': 2,\n",
    "'Phoebe Buffay': 3,\n",
    "'Rachel Green': 4,\n",
    "'Ross Geller': 5\n",
    "}\n",
    "\n",
    "\n",
    "y = df_tfidf.speaker.apply(lambda x: map_char_to_int[x]).values\n",
    "print(\"y shape\", y.shape)\n",
    "\n",
    "df_tfidf = pd.concat([df_tfidf[[\"speaker\", \"episode\"]],  pd.DataFrame(X)], axis=1)\n",
    "\n",
    "df_tfidf.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f01045",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d6b6f1",
   "metadata": {},
   "source": [
    "**2.2 —** To compare characters, carry out a classification task. Train a decision tree classifier to predict which main character uttered a sentence..\n",
    "\n",
    "A. Split the dataset into training and test sets using sklearn `sklearn.model_selection.train_test_split` using parameters `test_size=0.3` and `random_state=42`, and using the default values for all other parameters.\n",
    "\n",
    "B. Train a decision tree classifier (`sklearn.tree.DecisionTreeClassifier`) using `random_state=42`, leaving all other parameters as their default.\n",
    "\n",
    "C. Compute the accuracy of your classifier and of a random baseline, i.e., a classifier that predicts a character uniformly at random. **/Discuss:/** Compare the two accuracies.\n",
    "\n",
    "\n",
    "D. Compute the confusion matrix of your classifier using `sklearn.metrics.confusion_matrix`. Normalize the confusion matrix such that all cells sum to 1.\n",
    "\n",
    "E. Plot an appropriate graphical representation of the confusion matrix.\n",
    "\n",
    "F. **/Discuss:/** Analyzing the confusion matrix, discuss:\n",
    "   - Which character is most distinct in the way they talk?\n",
    "   - Which two characters are the most similar in the way they talk?\n",
    "   - Which two characters are the least similar in the way they talk?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c8bf67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a158a88",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**2.3 —** **/Discuss:/** Last, you discuss the results obtained in **2.2** with a friend, who asks you some thought-provoking questions.\n",
    "\n",
    "A. Your friend proposes that you should create a measure of similarity between two characters in a given episode in a more direct way than what you've done in **2.2**.  Propose (but do not implement) said similarity metric.\n",
    "\n",
    "B. Your friend also suggests that your analysis might not truly capture how two characters differ. According to her, if people are in the same conversation, they might speak similarly simply because they are in the same social context. Propose (but do not implement) a way of creating a dataset where this confounder does not exist.\n",
    "\n",
    "C. Last, your friend complains about how you present your (normalized) confusion matrix. According to her, from reading the cells alone, it is unclear if the fraction of occurrences is higher or lower than what a random classifier would yield. Propose (but do not implement) a way of modifying the confusion matrix to address her concern.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4253a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "25c65d90",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
